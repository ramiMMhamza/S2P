# Obtained from: https://github.com/open-mmlab/OpenUnReID
# Modified to support OUDA

import copy
import warnings
import random
import torch
import numpy as np
from torch.utils.data import DataLoader

from ..utils.dist_utils import get_dist_info
from .datasets import build_dataset
from .samplers import build_test_sampler, build_train_sampler
from .transformers import build_test_transformer, build_train_transformer
from .utils.dataset_wrapper import IterLoader, JointDataset
import torchvision.transforms as T

__all__ = ["build_train_dataloader", "build_val_dataloader","build_val_dataloader_source", "build_test_dataloader","build_train_dataloader_for_sim"]

# For reproducibility : 
def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

g = torch.Generator()
g.manual_seed(0)

def build_train_dataloader(
    cfg, n_tasks, task_id,filtre, pseudo_labels=None, datasets=None, epoch=0, joint=True,for_kd=False,only_source=False,
     **kwargs
):
    """
    Build training data loader
    """
    print("appel a build_train_dataloader")
    rank, world_size, dist = get_dist_info()

    data_root = cfg.DATA_ROOT  # PATH, str

    dataset_names = list(cfg.TRAIN.datasets.keys())  # list of str
    dataset_modes = list(cfg.TRAIN.datasets.values())  # list of str
    for mode in dataset_modes:
        assert mode in [
            "train",
            "trainval",
        ], "subset for training should be selected in [train, trainval]"
    unsup_dataset_indexes = cfg.TRAIN.unsup_dataset_indexes  # list or None
    filtration = filtre #  # list or None of dataset to divide into tasks

    if datasets is None:
        # generally for the first epoch
        if unsup_dataset_indexes is None:
            print(
                f"The training is in a fully-supervised manner with "
                f"{len(dataset_names)} dataset(s) ({dataset_names})"
            )
        else:
            no_label_datasets = [dataset_names[i] for i in unsup_dataset_indexes]
            print(
                f"The training is in a un/semi-supervised manner with "
                f"{len(dataset_names)} dataset(s) ({dataset_names}),\n"
                f"where {no_label_datasets} have no labels."
            )

        # build transformer
        train_transformer = build_train_transformer(cfg)

        # build individual datasets
        datasets = []
        if only_source:
            dataset_names= [dataset_names[-1]]
            dataset_modes= [dataset_modes[-1]]
        for idx, (dn, dm) in enumerate(zip(dataset_names, dataset_modes)):
            if only_source:
                idx = 1
            print(idx)
            filtre = filtration[idx]
            if unsup_dataset_indexes is None:
                datasets.append(
                    build_dataset(
                        dn, data_root, dm, n_tasks, task_id, filtre, del_labels=False, transform=train_transformer
                    )
                )
            else:
                if idx not in unsup_dataset_indexes or for_kd==True:
                    datasets.append(
                        build_dataset(
                            dn,
                            data_root,
                            dm,
                            n_tasks,
                            task_id,
                            filtre,
                            del_labels=False,
                            transform=train_transformer,
                        )
                    )
                    
                else:
                    try:
                        new_labels = pseudo_labels[unsup_dataset_indexes.index(idx)]
                    except Exception:
                        new_labels = None
                        warnings.warn("No labels are provided for {}.".format(dn))

                    datasets.append(
                        build_dataset(
                            dn,
                            data_root,
                            dm,
                            n_tasks,
                            task_id,
                            filtre,
                            pseudo_labels=new_labels,
                            del_labels=True,
                            transform=train_transformer,
                        )
                    )

    else:
        # update pseudo labels for unsupervised datasets
        for i, idx in enumerate(unsup_dataset_indexes):
            datasets[idx].renew_labels(pseudo_labels[i])

    if joint:
        # build joint datasets
        combined_datasets = JointDataset(datasets)
    else:
        combined_datasets = copy.deepcopy(datasets)

    # build sampler
    train_sampler = build_train_sampler(cfg, combined_datasets, epoch=epoch)

    # build data loader
    if dist:
        batch_size = cfg.TRAIN.LOADER.samples_per_gpu
        num_workers = cfg.TRAIN.LOADER.workers_per_gpu
    else:
        batch_size = cfg.TRAIN.LOADER.samples_per_gpu * cfg.total_gpus
        num_workers = cfg.TRAIN.LOADER.workers_per_gpu * cfg.total_gpus
    if joint:
        # a joint data loader
        return (
            IterLoader(
                DataLoader(
                    combined_datasets,
                    batch_size=batch_size,
                    num_workers=num_workers,
                    sampler=train_sampler,
                    shuffle=False,
                    pin_memory=True,
                    drop_last=True,
                    worker_init_fn=seed_worker,
                    generator=g,
                    **kwargs,
                ),
                length=cfg.TRAIN.iters,
            ),
            datasets,
        )

    else:
        # several individual data loaders
        data_loaders = []
        for dataset, sampler in zip(combined_datasets, train_sampler):
            data_loaders.append(
                IterLoader(
                    DataLoader(
                        dataset,
                        batch_size=batch_size,
                        num_workers=num_workers,
                        sampler=sampler,
                        shuffle=False,
                        pin_memory=True,
                        drop_last=True,
                        worker_init_fn=seed_worker,
                        generator=g,
                        **kwargs,
                    ),
                    length=cfg.TRAIN.iters,
                )
            )

        return data_loaders, datasets

def build_train_dataloader_for_sim(
    cfg, n_tasks, task_id,filtration, pseudo_labels=None, datasets=None, epoch=0, joint=True, **kwargs
):
    """
    Build training data loader
    """
    print("appel a build_train_dataloader")
    rank, world_size, dist = get_dist_info()

    data_root = cfg.DATA_ROOT  # PATH, str

    dataset_names = list(cfg.TRAIN.datasets.keys())  # list of str
    dataset_modes = list(cfg.TRAIN.datasets.values())  # list of str
    for mode in dataset_modes:
        assert mode in [
            "train",
            "trainval",
        ], "subset for training should be selected in [train, trainval]"
    unsup_dataset_indexes = cfg.TRAIN.unsup_dataset_indexes  # list or None

    if datasets is None:
        # generally for the first epoch
        if unsup_dataset_indexes is None:
            print(
                f"The training is in a fully-supervised manner with "
                f"{len(dataset_names)} dataset(s) ({dataset_names})"
            )
        else:
            no_label_datasets = [dataset_names[i] for i in unsup_dataset_indexes]
            print(
                f"The training is in a un/semi-supervised manner with "
                f"{len(dataset_names)} dataset(s) ({dataset_names}),\n"
                f"where {no_label_datasets} have no labels."
            )

        # build transformer
        res = []
        #resize 
        res.append(T.Resize((256, 128), interpolation=3))
        #to tensor
        res.append(T.ToTensor())
        train_transformer = T.Compose(res)
        # build individual datasets
        datasets = []
        for idx, (dn, dm) in enumerate(zip(dataset_names, dataset_modes)):
            filtre = filtration[idx]
            if unsup_dataset_indexes is None:
                datasets.append(
                    build_dataset(
                        dn, data_root, dm, n_tasks, task_id, filtre, del_labels=False, transform=train_transformer
                    )
                )
            else:
                if idx not in unsup_dataset_indexes:
                    datasets.append(
                        build_dataset(
                            dn,
                            data_root,
                            dm,
                            n_tasks,
                            task_id,
                            filtre,
                            del_labels=False,
                            transform=train_transformer,
                        )
                    )
                else:
                    try:
                        new_labels = pseudo_labels[unsup_dataset_indexes.index(idx)]
                    except Exception:
                        new_labels = None
                        warnings.warn("No labels are provided for {}.".format(dn))

                    datasets.append(
                        build_dataset(
                            dn,
                            data_root,
                            dm,
                            n_tasks,
                            task_id,
                            filtre,
                            pseudo_labels=new_labels,
                            del_labels=True,
                            transform=train_transformer,
                        )
                    )

    else:
        # update pseudo labels for unsupervised datasets
        for i, idx in enumerate(unsup_dataset_indexes):
            datasets[idx].renew_labels(pseudo_labels[i])

    if joint:
        # build joint datasets
        combined_datasets = JointDataset(datasets)
    else:
        combined_datasets = copy.deepcopy(datasets)

    # build sampler
    train_sampler = build_train_sampler(cfg, combined_datasets, epoch=epoch)

    # build data loader
    if dist:
        batch_size = cfg.TRAIN.LOADER.samples_per_gpu
        num_workers = cfg.TRAIN.LOADER.workers_per_gpu
    else:
        batch_size = cfg.TRAIN.LOADER.samples_per_gpu * cfg.total_gpus
        num_workers = cfg.TRAIN.LOADER.workers_per_gpu * cfg.total_gpus
    if joint:
        # a joint data loader
        return (
            IterLoader(
                DataLoader(
                    combined_datasets,
                    batch_size=batch_size,
                    num_workers=num_workers,
                    sampler=train_sampler,
                    shuffle=False,
                    pin_memory=True,
                    drop_last=True,
                    worker_init_fn=seed_worker,
                    generator=g,
                    **kwargs,
                ),
                length=cfg.TRAIN.iters_for_sim,
            ),
            datasets,
        )

    else:
        # several individual data loaders
        data_loaders = []
        for dataset, sampler in zip(combined_datasets, train_sampler):
            data_loaders.append(
                IterLoader(
                    DataLoader(
                        dataset,
                        batch_size=batch_size,
                        num_workers=num_workers,
                        sampler=sampler,
                        shuffle=False,
                        pin_memory=True,
                        drop_last=True,
                        worker_init_fn=seed_worker,
                        generator=g,
                        **kwargs,
                    ),
                    length=cfg.TRAIN.iters_for_sim,
                )
            )

        return data_loaders, datasets

def build_train_dataloader_for_sim_old(
    cfg, n_tasks, task_id,filtre, pseudo_labels=None, datasets=None, epoch=0, joint=True, **kwargs
):
    """
    Build training data loader
    """
    print("appel a build_train_dataloader for sim")
    rank, world_size, dist = get_dist_info()

    data_root = cfg.DATA_ROOT  # PATH, str

    dataset_names = list(cfg.TRAIN.datasets.keys())  # list of str
    dataset_modes = list(cfg.TRAIN.datasets.values())  # list of str
    for mode in dataset_modes:
        assert mode in [
            "train",
            "trainval",
        ], "subset for training should be selected in [train, trainval]"
    unsup_dataset_indexes = cfg.TRAIN.unsup_dataset_indexes  # list or None
    filtration = cfg.TRAIN.filtre_sim  # list or None of dataset to divide into tasks

    if datasets is None:
        # generally for the first epoch
        if unsup_dataset_indexes is None:
            print(
                f"The training is in a fully-supervised manner with "
                f"{len(dataset_names)} dataset(s) ({dataset_names})"
            )
        else:
            no_label_datasets = [dataset_names[i] for i in unsup_dataset_indexes]
            print(
                f"The training is in a un/semi-supervised manner with "
                f"{len(dataset_names)} dataset(s) ({dataset_names}),\n"
                f"where {no_label_datasets} have no labels."
            )

        # build transformer
        res = []
        #resize 
        res.append(T.Resize((256, 128), interpolation=3))
        #to tensor
        res.append(T.ToTensor())
        train_transformer = T.Compose(res) 

        # build individual datasets
        datasets = []
        for idx, (dn, dm) in enumerate(zip(dataset_names, dataset_modes)):
            filtre = filtration[idx]
            if unsup_dataset_indexes is None:
                datasets.append(
                    build_dataset(
                        dn, data_root, dm, n_tasks, task_id, filtre, del_labels=False, transform=train_transformer
                    )
                )
            else:
                if idx not in unsup_dataset_indexes:
                    datasets.append(
                        build_dataset(
                            dn,
                            data_root,
                            dm,
                            n_tasks,
                            task_id,
                            filtre,
                            del_labels=False,
                            transform=train_transformer,
                        )
                    )
                else:
                    try:
                        new_labels = pseudo_labels[unsup_dataset_indexes.index(idx)]
                    except Exception:
                        new_labels = None
                        warnings.warn("No labels are provided for {}.".format(dn))

                    datasets.append(
                        build_dataset(
                            dn,
                            data_root,
                            dm,
                            n_tasks,
                            task_id,
                            filtre,
                            pseudo_labels=new_labels,
                            del_labels=True,
                            transform=train_transformer,
                        )
                    )

    else:
        # update pseudo labels for unsupervised datasets
        for i, idx in enumerate(unsup_dataset_indexes):
            datasets[idx].renew_labels(pseudo_labels[i])

    if joint:
        # build joint datasets
        combined_datasets = JointDataset(datasets)
    else:
        combined_datasets = copy.deepcopy(datasets)

    # build sampler
    train_sampler = build_train_sampler(cfg, combined_datasets, epoch=epoch)

    # build data loader
    if dist:
        batch_size = cfg.TRAIN.LOADER.samples_per_gpu
        num_workers = cfg.TRAIN.LOADER.workers_per_gpu
    else:
        batch_size = cfg.TRAIN.LOADER.samples_per_gpu * cfg.total_gpus
        num_workers = cfg.TRAIN.LOADER.workers_per_gpu * cfg.total_gpus
    if joint:
        # a joint data loader
        return (
                DataLoader(
                    combined_datasets,
                    batch_size=batch_size,
                    num_workers=num_workers,
                    sampler=train_sampler,
                    shuffle=False,
                    pin_memory=True,
                    drop_last=True,
                    worker_init_fn=seed_worker,
                    generator=g,
                    **kwargs,
                ),
            datasets,
        )

    else:
        # several individual data loaders
        data_loaders = []
        for dataset, sampler in zip(combined_datasets, train_sampler):
            data_loaders.append(
                    DataLoader(
                        dataset,
                        batch_size=batch_size,
                        num_workers=num_workers,
                        sampler=sampler,
                        shuffle=False,
                        pin_memory=True,
                        drop_last=True,
                        worker_init_fn=seed_worker,
                        generator=g,
                        **kwargs,
                    ),
                    length=cfg.TRAIN.iters,
                
            )

        return data_loaders, datasets

def build_val_dataloader_source(
    cfg, n_tasks = None, task_id = None, for_clustering=False, all_datasets=False, one_gpu=False, **kwargs
):
    """
    Build validation data loader
    it can be also used for clustering
    """
    print("appel build_val_dataloader source")
    n_tasks = cfg.n_tasks
    if task_id == None or task_id==-1:
        task_id = cfg.task_id
        cfg.TRAIN.sup_dataset_indexes = [0,]
        filtre = True
    else:
        cfg.TRAIN.sup_dataset_indexes = [0,]
        filtre = True
    rank, world_size, dist = get_dist_info()

    data_root = cfg.DATA_ROOT  # PATH, str
    dataset_names = list(cfg.TRAIN.datasets.keys())  # list of str
    filtration = cfg.TRAIN.filtre  # list or None of dataset to divide into tasks
    if for_clustering:
        dataset_modes = list(cfg.TRAIN.datasets.values())  # list of str
        if all_datasets:
            sup_dataset_indexes = list(np.arange(len(dataset_names)))
        else:
            sup_dataset_indexes = cfg.TRAIN.sup_dataset_indexes  # list or None
        assert sup_dataset_indexes is not None, "all datasets are fully-supervised"
        dataset_names = [dataset_names[idx] for idx in sup_dataset_indexes]
        dataset_modes = ["trainval" for idx in sup_dataset_indexes]
        # filtre = False
    else:
        dataset_names = [cfg.TRAIN.val_dataset]
        dataset_modes = ["val"] * len(dataset_names)
        filtre = False
    # build transformer
    test_transformer = build_test_transformer(cfg)

    # build individual datasets
    datasets, vals = [], []
    for dn, dm in zip(dataset_names, dataset_modes):
        val_data = build_dataset(
            dn,
            data_root,
            dm,
            n_tasks,
            task_id,
            filtre,
            del_labels=False,
            transform=test_transformer,
            verbose=(not for_clustering),
        )
        datasets.append(val_data)
        vals.append(val_data.data)

    # build sampler
    if not one_gpu:
        test_sampler = build_test_sampler(cfg, datasets)
    else:
        test_sampler = [None] * len(datasets)

    # build data loader
    if dist:
        batch_size = cfg.TEST.LOADER.samples_per_gpu
        num_workers = cfg.TEST.LOADER.workers_per_gpu
    else:
        batch_size = cfg.TEST.LOADER.samples_per_gpu * cfg.total_gpus
        num_workers = cfg.TEST.LOADER.workers_per_gpu * cfg.total_gpus

    # several individual data loaders
    data_loaders = []
    for dataset, sampler in zip(datasets, test_sampler):
        data_loaders.append(
            DataLoader(
                dataset,
                batch_size=batch_size,
                num_workers=num_workers,
                sampler=sampler,
                shuffle=False,
                pin_memory=True,
                drop_last=False,
                worker_init_fn=seed_worker,
                generator=g,
                **kwargs,
            )
        )
    return data_loaders, datasets

def build_val_dataloader(
    cfg, n_tasks = None, task_id = None, for_clustering=False, all_datasets=False, one_gpu=False,SpCL = False, **kwargs
):
    """
    Build validation data loader
    it can be also used for clustering
    """
    print("appel build valdataloader")
    n_tasks = cfg.n_tasks
    if task_id == None:
        task_id = cfg.task_id
    rank, world_size, dist = get_dist_info()

    data_root = cfg.DATA_ROOT  # PATH, str
    dataset_names = list(cfg.TRAIN.datasets.keys())  # list of str
    filtration = cfg.TRAIN.filtre  # list or None of dataset to divide into tasks
    if for_clustering:
        dataset_modes = list(cfg.TRAIN.datasets.values())  # list of str
        if all_datasets:
            unsup_dataset_indexes = list(np.arange(len(dataset_names)))
        else:
            unsup_dataset_indexes = cfg.TRAIN.unsup_dataset_indexes  # list or None
        assert unsup_dataset_indexes is not None, "all datasets are fully-supervised"
        dataset_names = [dataset_names[idx] for idx in unsup_dataset_indexes]
        dataset_modes = [dataset_modes[idx] for idx in unsup_dataset_indexes]
        if SpCL:
            filtre = [True,False] #cfg.TRAIN.filtre #
        else: 
            filtre = True
    else:
        dataset_names = [cfg.TRAIN.val_dataset]
        dataset_modes = ["val"] * len(dataset_names)
        filtre = False
    # build transformer
    test_transformer = build_test_transformer(cfg)

    # build individual datasets
    datasets, vals = [], []
    if SpCL:
        for ind, (dn, dm) in enumerate(zip(dataset_names, dataset_modes)):
            val_data = build_dataset(
                dn,
                data_root,
                dm,
                n_tasks,
                task_id,
                filtre[ind],
                del_labels=False,
                transform=test_transformer,
                verbose=(not for_clustering),
            )
            datasets.append(val_data)
            vals.append(val_data.data)
    else:
        for dn, dm in zip(dataset_names, dataset_modes):
            val_data = build_dataset(
                dn,
                data_root,
                dm,
                n_tasks,
                task_id,
                filtre,
                del_labels=False,
                transform=test_transformer,
                verbose=(not for_clustering),
            )
            datasets.append(val_data)
            vals.append(val_data.data)

    # build sampler
    if not one_gpu:
        test_sampler = build_test_sampler(cfg, datasets)
    else:
        test_sampler = [None] * len(datasets)

    # build data loader
    if dist:
        batch_size = cfg.TEST.LOADER.samples_per_gpu
        num_workers = cfg.TEST.LOADER.workers_per_gpu
    else:
        batch_size = cfg.TEST.LOADER.samples_per_gpu * cfg.total_gpus
        num_workers = cfg.TEST.LOADER.workers_per_gpu * cfg.total_gpus

    # several individual data loaders
    data_loaders = []
    for dataset, sampler in zip(datasets, test_sampler):
        data_loaders.append(
            DataLoader(
                dataset,
                batch_size=batch_size,
                num_workers=num_workers,
                sampler=sampler,
                shuffle=False,
                pin_memory=True,
                drop_last=False,
                worker_init_fn=seed_worker,
                generator=g,
                **kwargs,
            )
        )
    return data_loaders, vals
def build_test_dataloader(cfg, n_tasks = None, task_id=None,  one_gpu=False, **kwargs):
    """
    Build testing data loader
    """
    print("appel build teset dataloader")
    rank, world_size, dist = get_dist_info()

    data_root = cfg.DATA_ROOT  # PATH, str
    dataset_names = cfg.TEST.datasets  # list of str

    # build transformer
    test_transformer = build_test_transformer(cfg)

    # build individual datasets
    datasets, queries, galleries = [], [], []
    for dn in dataset_names:
        query_data = build_dataset(
            dn, data_root, "query",n_tasks, task_id, False, del_labels=False, transform=test_transformer
        )
        gallery_data = build_dataset(
            dn, data_root, "gallery", n_tasks, task_id, False, del_labels=False, transform=test_transformer
        )
        datasets.append(query_data + gallery_data)
        queries.append(query_data.data)
        galleries.append(gallery_data.data)

    # build sampler
    if not one_gpu:
        test_sampler = build_test_sampler(cfg, datasets)
    else:
        test_sampler = [None] * len(datasets)

    # build data loader
    if dist:
        batch_size = cfg.TEST.LOADER.samples_per_gpu
        num_workers = cfg.TEST.LOADER.workers_per_gpu
    else:
        batch_size = cfg.TEST.LOADER.samples_per_gpu * cfg.total_gpus
        num_workers = cfg.TEST.LOADER.workers_per_gpu * cfg.total_gpus

    # several individual data loaders
    data_loaders = []
    for dataset, sampler in zip(datasets, test_sampler):
        data_loaders.append(
            DataLoader(
                dataset,
                batch_size=batch_size,
                num_workers=num_workers,
                sampler=sampler,
                shuffle=False,
                pin_memory=True,
                drop_last=False,
                **kwargs,
            )
        )

    return data_loaders, queries, galleries
